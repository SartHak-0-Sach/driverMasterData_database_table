{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.append('../utils')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from queryHelper import prodFetch, adbFetch\n",
    "from databaseHelper import *\n",
    "from sheetHelper import *\n",
    "from datetime import timedelta\n",
    "import psycopg2\n",
    "import pygsheets as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2025-05-01')\n",
    "yesterday_date = pd.Timestamp.today().date() - timedelta(days=1)\n",
    "\n",
    "dfDriverHistories = adbFetch(f\"\"\"select driverId, date, status dayEndStatus, operatorId, zoneId, vehicleType, clientId, liveDate, cashWallet/100 cashWallet, pointsWallet/100 pointsWallet, penaltyWallet/100 penaltyWallet, isDefaulter, nonOpsDays from dailyDriversHistories force index(dailyDriversHistories_date_driverId_unique) where date between '{start_date}' and '{yesterday_date}' and isBaasDriver != 1 and status not in ('left', 'deleted', 'terminated') and driverId like 'D%' \"\"\")\n",
    "\n",
    "dfDriverHistories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDriverHistories.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTxns = prodFetch(f\"\"\" select sum(upiCollection) upiCollection, date txnDate, driverId txnDriverId, sum(penalty) penalty, sum(netGMV)/100 netGMV, sum(walletCashUsed)/100 walletCashUsed, sum(pointsUsed)/100 pointUsed, count(*) numberOfSwaps, SUM(CASE WHEN qrType = 'smartCard' THEN 1 ELSE 0 END) AS smartCardTransactionCount, SUM(CASE WHEN qrType = 'vpa' THEN 1 ELSE 0 END) AS vpaTransactionCount from transactions force index(transaction_date_index) where date between '{start_date}' and '{yesterday_date}' and deletedAt is null group by txnDriverId, txnDate \"\"\")\n",
    "\n",
    "dfTxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTxns['txnDate'] = pd.to_datetime(dfTxns['txnDate']).dt.date\n",
    "\n",
    "dfTxnsToday = dfTxns[dfTxns['txnDate'] == (yesterday_date)]\n",
    "\n",
    "dfTxnsToday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDailyDriverHistories = dfDriverHistories[dfDriverHistories['date'] == (yesterday_date)]\n",
    "\n",
    "dfDailyDriverHistories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf = dfDailyDriverHistories.merge(dfTxnsToday, left_on='driverId', right_on='txnDriverId', how='left', indicator=True).drop(columns = ['txnDriverId', 'txnDate'])\n",
    "\n",
    "dailyDf['status'] = dailyDf.apply(lambda row: 'active' if row['dayEndStatus'] != 'active' and row['_merge'] == 'both' else row['dayEndStatus'], axis=1)\n",
    "\n",
    "dailyDf.drop(columns = ['_merge', 'dayEndStatus'], inplace=True)\n",
    "\n",
    "dailyDf.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = dfDriverHistories.merge(dfTxns, left_on=['driverId', 'date'], right_on=['txnDriverId', 'txnDate'], how='left', indicator=True).drop(columns=['txnDriverId', 'txnDate'])\n",
    "\n",
    "overallDf['status'] = overallDf.apply(lambda row: 'active' if row['dayEndStatus'] != 'active' and row['_merge'] == 'both' else row['dayEndStatus'], axis=1)\n",
    "\n",
    "overallDf.drop(columns = ['_merge', 'dayEndStatus'], inplace=True)\n",
    "\n",
    "overallDf.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeatureWallet = prodFetch(f\"\"\"select date, cashAmount/100 cashAmount, pointAmount/100 pointsamount, driverId, featureOrderId from featureDriverWallets where date between '{start_date}' and '{yesterday_date}' and deletedAt is null and paymentFor in ('walletRecharge', 'driverKhataRecharge') \"\"\")\n",
    "\n",
    "dfFeatureWallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeatureOrders = prodFetch(f\"\"\"select id, orderCreatedFor, orderCreatedBy, DATE(DATE_ADD(createdAt, INTERVAL 330 MINUTE)) orderDate from featureOrders force index(orderCreatedFor) where deletedAt is null and DATE(DATE_ADD(createdAt, INTERVAL 330 MINUTE)) between '{start_date}' and '{yesterday_date}' and orderCreatedFor is not null and orderCreatedFor like 'D%'\"\"\")  \n",
    "\n",
    "dfFeatureOrders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = dfFeatureOrders.merge(dfFeatureWallet, left_on=['id', 'orderDate'], right_on=['featureOrderId', 'date'], how='inner')\n",
    "\n",
    "df_result = df_result[dfFeatureWallet.columns]\n",
    "\n",
    "df_result.drop(columns=['featureOrderId'], inplace=True)\n",
    "\n",
    "df_result['WalletRechargeCount'] = 1\n",
    "\n",
    "df_result.groupby(['driverId', 'date']).agg({'cashAmount': 'sum', 'pointsamount': 'sum', 'WalletRechargeCount': 'sum'}).reset_index()\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.groupby(['driverId', 'date']).agg({'cashAmount': 'sum', 'pointsamount': 'sum', 'WalletRechargeCount': 'sum'}).reset_index().rename(columns={'cashAmount': 'walletRechargeCashAmount', 'pointsamount': 'walletRechargePointsAmount', 'WalletRechargeCount': 'walletRechargeCount'})\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['date'] = pd.to_datetime(df_result['date']).dt.date\n",
    "\n",
    "df_resultDaily = df_result[df_result['date'] == yesterday_date]\n",
    "\n",
    "df_resultDaily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dailyDf = dailyDf.merge(df_resultDaily, on=['driverId', 'date'], how='left')\n",
    "\n",
    "# dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overallDf = overallDf.merge(df_result, on=['driverId', 'date'], how='left')\n",
    "\n",
    "# overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCards = prodFetch(f\"\"\"select occupant, status smartCardStatus, createdAt, id smartCardId from smartCards where deletedAt is null and status = 'active' \"\"\")\n",
    "\n",
    "dfCards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCards.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latest_id = dfCards.loc[dfCards.groupby('occupant')['createdAt'].idxmax(), ['occupant', 'smartCardId']]\n",
    "\n",
    "df_latest_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf = dailyDf.merge(df_latest_id, left_on='driverId', right_on='occupant', how='left').drop(columns=['occupant'])\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf['smartCardOccupancyFlag'] = dailyDf.apply(lambda x: 1 if pd.notna(x['smartCardId']) else 0, axis=1)\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCardsStatusLogs = prodFetch(f\"\"\"\n",
    "    SELECT smartCardId, status, driverId, date\n",
    "    FROM (\n",
    "        SELECT \n",
    "            smartCardId,\n",
    "            status,\n",
    "            occupant AS driverId,\n",
    "            date(DATE_ADD(createdAt, INTERVAL 330 MINUTE)) AS date,\n",
    "            ROW_NUMBER() OVER (PARTITION BY smartCardId ORDER BY date(DATE_ADD(createdAt, INTERVAL 330 MINUTE)) DESC) AS rn\n",
    "        FROM smartCardStatusLogs\n",
    "        WHERE date(DATE_ADD(createdAt, INTERVAL 330 MINUTE)) \n",
    "            BETWEEN '{start_date}' AND '{yesterday_date}'\n",
    "    ) AS sub\n",
    "    WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "dfCardsStatusLogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = overallDf.merge(df_latest_id, left_on='driverId', right_on='occupant', how='left').drop(columns=['occupant'])\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf['smartCardOccupancyFlag'] = overallDf.apply(lambda x: 1 if pd.notna(x['smartCardId']) else 0, axis=1)\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLoyalPartner = prodFetch(f\"\"\"\n",
    "WITH filteredTransactions AS (\n",
    "    SELECT \n",
    "        driverId, \n",
    "        partnerId, \n",
    "        COUNT(*) AS txns, \n",
    "        MAX(createdAt) AS lastTxnTime\n",
    "    FROM transactions\n",
    "    WHERE \n",
    "        createdAt >= DATE_SUB(CURDATE(), INTERVAL 28 DAY)\n",
    "        AND driverId LIKE 'D%'\n",
    "        AND deletedAt IS NULL\n",
    "    GROUP BY driverId, partnerId\n",
    "),\n",
    "rankedPartners AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY driverId \n",
    "            ORDER BY txns DESC, lastTxnTime DESC\n",
    "        ) AS rn\n",
    "    FROM filteredTransactions\n",
    ")\n",
    "SELECT \n",
    "    driverId, \n",
    "    partnerId AS StickyPartnerId\n",
    "FROM rankedPartners\n",
    "WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "dfLoyalPartner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfLoyalPartner = prodFetch(f\"\"\"\n",
    "# WITH FilteredTransactions AS (\n",
    "#     SELECT\n",
    "#         driverId,\n",
    "#         partnerId,\n",
    "#         COUNT(*) AS Transactions\n",
    "#     FROM\n",
    "#         transactions\n",
    "#     WHERE\n",
    "#         createdAt >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)\n",
    "#         AND driverId like 'D%'\n",
    "#         AND deletedAt IS NULL\n",
    "#     GROUP BY\n",
    "#         driverId, partnerId\n",
    "# ),\n",
    "# DriverTotals AS (\n",
    "#     SELECT\n",
    "#         driverId,\n",
    "#         SUM(Transactions) AS TotalTransactions\n",
    "#     FROM\n",
    "#         FilteredTransactions\n",
    "#     GROUP BY\n",
    "#         driverId\n",
    "# ),\n",
    "# StickyPartner AS (\n",
    "#     SELECT\n",
    "#         ft.driverId,\n",
    "#         ft.partnerId AS StickyPartnerId,\n",
    "#         ft.Transactions AS StickyPartnerTransactions\n",
    "#     FROM\n",
    "#         FilteredTransactions ft\n",
    "#     JOIN (\n",
    "#         SELECT\n",
    "#             driverId,\n",
    "#             MAX(Transactions) AS MaxTransactions\n",
    "#         FROM\n",
    "#             FilteredTransactions\n",
    "#         GROUP BY\n",
    "#             driverId\n",
    "#     ) MaxTransactionsTable\n",
    "#     ON\n",
    "#         ft.driverId = MaxTransactionsTable.driverId\n",
    "#         AND ft.Transactions = MaxTransactionsTable.MaxTransactions\n",
    "# )\n",
    "# SELECT\n",
    "#     sp.driverId,\n",
    "#     sp.StickyPartnerId\n",
    "# FROM\n",
    "#     StickyPartner sp\n",
    "# JOIN\n",
    "#     DriverTotals dt\n",
    "# ON\n",
    "#     sp.driverId = dt.driverId\n",
    "# WHERE\n",
    "#     (sp.StickyPartnerTransactions / dt.TotalTransactions) >= 0.75\n",
    "# \"\"\")\n",
    "\n",
    "# dfLoyalPartner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAllPartnersWithPercent = prodFetch(f\"\"\"\n",
    "# WITH FilteredTransactions AS (\n",
    "#     SELECT\n",
    "#         driverId,\n",
    "#         partnerId,\n",
    "#         COUNT(*) AS Transactions\n",
    "#     FROM\n",
    "#         transactions\n",
    "#     WHERE\n",
    "#         createdAt >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)\n",
    "#         AND driverId like 'D%'\n",
    "#         AND deletedAt IS NULL\n",
    "#     GROUP BY\n",
    "#         driverId, partnerId\n",
    "# ),\n",
    "# DriverTotals AS (\n",
    "#     SELECT\n",
    "#         driverId,\n",
    "#         SUM(Transactions) AS TotalTransactions\n",
    "#     FROM\n",
    "#         FilteredTransactions\n",
    "#     GROUP BY\n",
    "#         driverId\n",
    "# )\n",
    "# SELECT\n",
    "#     ft.driverId,\n",
    "#     ft.partnerId,\n",
    "#     ft.Transactions,\n",
    "#     dt.TotalTransactions,\n",
    "#     ROUND((ft.Transactions / dt.TotalTransactions) * 100, 2) AS PercentageOfTransactions\n",
    "# FROM\n",
    "#     FilteredTransactions ft\n",
    "# JOIN\n",
    "#     DriverTotals dt\n",
    "# ON\n",
    "#     ft.driverId = dt.driverId\n",
    "# ORDER BY\n",
    "#     ft.driverId, PercentageOfTransactions DESC\n",
    "# \"\"\")\n",
    "\n",
    "# dfAllPartnersWithPercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLoyalPartner.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfLoyalPartnerGrouped = dfLoyalPartner.groupby('driverId')['StickyPartnerId'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "\n",
    "# dfLoyalPartnerGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf = dailyDf.merge(dfLoyalPartner, on='driverId', how='left')\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLoyalPartnerPast = prodFetch(f\"\"\"\n",
    "WITH RECURSIVE dateSeries AS (\n",
    "    SELECT DATE('{start_date}') AS date\n",
    "    UNION ALL\n",
    "    SELECT DATE_ADD(date, INTERVAL 1 DAY)\n",
    "    FROM dateSeries\n",
    "    WHERE date <= '{yesterday_date}'\n",
    "),\n",
    "filteredTransactions AS (\n",
    "    SELECT \n",
    "        ds.date,\n",
    "        t.driverId, \n",
    "        t.partnerId, \n",
    "        COUNT(*) AS txns, \n",
    "        MAX(t.createdAt) AS lastTxnTime\n",
    "    FROM dateSeries ds\n",
    "    JOIN transactions t \n",
    "        ON t.createdAt >= DATE_SUB(ds.date, INTERVAL 28 DAY)\n",
    "        AND t.createdAt < ds.date\n",
    "        AND t.driverId LIKE 'D%'\n",
    "        AND t.deletedAt IS NULL\n",
    "    GROUP BY ds.date, t.driverId, t.partnerId\n",
    "),\n",
    "rankedPartners AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY date, driverId \n",
    "            ORDER BY txns DESC, lastTxnTime DESC\n",
    "        ) AS rn\n",
    "    FROM filteredTransactions\n",
    ")\n",
    "SELECT \n",
    "    date,\n",
    "    driverId, \n",
    "    partnerId AS StickyPartnerId\n",
    "FROM rankedPartners\n",
    "WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "dfLoyalPartnerPast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLoyalPartnerPast.sort_values(by=['driverId', 'date'], ascending=[True, True], inplace=True)\n",
    "dfLoyalPartnerPast['date'] = pd.to_datetime(dfLoyalPartnerPast['date']).dt.date\n",
    "\n",
    "dfLoyalPartnerPast.reset_index(drop=True, inplace=True)\n",
    "dfLoyalPartnerPast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = overallDf.merge(dfLoyalPartnerPast, on=['driverId', 'date'], how='left')\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf.replace(np.nan, \"\", inplace=True)\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf.replace(np.nan, \"\", inplace=True)\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname = \"operations_manager_prod\", user = \"sarthak_sachdev\", password = \"Sarthak@12345\", host = \"operation.replica.upgrid.in\", port = \"5432\")\n",
    "\n",
    "print('PostgreSQL Connection Established')\n",
    "\n",
    "# Fetch Data from PostgreSQL server\n",
    "query1 = f'''select complainant_id, (created_at + INTERVAL '330 minutes')::DATE as ticket_date, count(*) totalTickets from tickets where (created_at + INTERVAL '330 minutes')::DATE between '{start_date}' and '{yesterday_date}' and deleted_at is null and complainant_id like 'D%' group by complainant_id, ticket_date\n",
    "'''\n",
    "\n",
    "dfTickets = pd.read_sql(query1, conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "dfTickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTickets.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTicketsDaily = dfTickets[dfTickets['ticket_date'] == (yesterday_date)]\n",
    "\n",
    "dfTicketsDaily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf = dailyDf.merge(dfTicketsDaily, left_on=['driverId', 'date'], right_on=['complainant_id', 'ticket_date'], how='left').drop(columns=['complainant_id', 'ticket_date'])\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = overallDf.merge(dfTickets, left_on=['driverId', 'date'], right_on=['complainant_id', 'ticket_date'], how='left').drop(columns=['complainant_id', 'ticket_date'])\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUsers = prodFetch(\"\"\"select employeeId, mobile, alternateMobile from users where employeeId like 'D%' group by 1 \"\"\")\n",
    "\n",
    "dfUsers.replace(np.nan, \"\", inplace=True)\n",
    "\n",
    "dfUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUsers.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCallingNumber = prodFetch(\"\"\" select customerId, callingNumber, DATE_ADD(createdAt, INTERVAL 330 MINUTE) AS callDate, count, createdAt from customerCallingNumbers where deletedAt is null\"\"\")\n",
    "\n",
    "dfCallingNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCallingNumber['date'] = pd.to_datetime(dfCallingNumber['callDate'])\n",
    "dfCallingNumber['createdAt'] = pd.to_datetime(dfCallingNumber['createdAt'])\n",
    "\n",
    "dfCallingNumber_sorted = dfCallingNumber.sort_values(by=['count', 'createdAt'], ascending=[False, False])\n",
    "\n",
    "df_filtered = dfCallingNumber_sorted.drop_duplicates(subset=['customerId'], keep='first')\n",
    "\n",
    "dfNumbers = dfUsers.merge(df_filtered, left_on='employeeId', right_on='customerId', how='left').drop(\n",
    "    columns=['customerId', 'count', 'callDate', 'date', 'createdAt']\n",
    ")\n",
    "\n",
    "dfNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNumbers.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNumbers['callingNumber'] = dfNumbers['callingNumber'].astype(str).str.replace('NaN', '')\n",
    "\n",
    "sample = dfNumbers[dfNumbers['callingNumber'] != \"nan\"].copy()\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNumbers.replace(np.nan, \"\", inplace=True)\n",
    "\n",
    "dfNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNumbers.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf = dailyDf.merge(dfNumbers, left_on='driverId', right_on='employeeId', how='left').drop(columns=['employeeId'])\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = overallDf.merge(dfNumbers, left_on='driverId', right_on='employeeId', how='left').drop(columns=['employeeId'])\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfLeaves = prodFetch(f\"\"\"SELECT driverId, date, SUM(1) OVER (PARTITION BY driverId ORDER BY date) as nonOpsDays FROM driverLeaves WHERE deletedAt IS NULL AND driverId LIKE 'D%' AND date >= '20250301' ORDER BY driverId, date\"\"\")\n",
    "\n",
    "# dfLeaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfTxn = prodFetch(f\"\"\"WITH UniqueTxns AS (SELECT DISTINCT driverId, date FROM transactions WHERE deletedAt IS NULL AND driverId LIKE 'D%' AND date >= '20250301') SELECT driverId, date, SUM(1) OVER (PARTITION BY driverId ORDER BY date) as OpsDays FROM UniqueTxns WHERE date >= '20250301' ORDER BY driverId, date\"\"\")\n",
    "\n",
    "# dfTxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfdates = adbFetch(f\"\"\" select driverId, date from dailyDriversHistories where date between '20250301' and '20250331' and isBaasDriver != 1 and status not in ('left', 'deleted', 'terminated') and driverId like 'D%' \"\"\")\n",
    "\n",
    "# dfdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfLeaves['date'] = pd.to_datetime(dfLeaves['date'])\n",
    "# dfTxn['date'] = pd.to_datetime(dfTxn['date'])\n",
    "# dfdates['date'] = pd.to_datetime(dfdates['date'])\n",
    "\n",
    "# dfLeaves_full = dfdates.merge(dfLeaves, on=['date', 'driverId'], how='left')\n",
    "# dfTxn_full = dfdates.merge(dfTxn, on=['date', 'driverId'], how='left')\n",
    "\n",
    "# dfLeaves_full['nonOpsDays'] = dfLeaves_full.groupby('driverId')['nonOpsDays'].ffill().fillna(0)\n",
    "# dfTxn_full['OpsDays'] = dfTxn_full.groupby('driverId')['OpsDays'].ffill().fillna(0)\n",
    "\n",
    "# dfLeaves_full = dfLeaves_full.reset_index(drop=True)\n",
    "# dfTxn_full = dfTxn_full.reset_index(drop=True)\n",
    "\n",
    "# dfLeaves_full['driverId'] = dfLeaves_full['driverId'].ffill()\n",
    "# dfTxn_full['driverId'] = dfTxn_full['driverId'].ffill()\n",
    "\n",
    "# print(dfLeaves_full)\n",
    "# print(dfTxn_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfDsh = prodFetch(f\"\"\"select driverId, date, SUM(1) OVER (PARTITION BY driverId ORDER BY date) as activeDays from driverStatusHistories where driverId like 'D%' and date between '20250301' and '20250331' and deletedAt is null and status in ('active')\"\"\")\n",
    "\n",
    "# dfDsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfDsh['date'] = pd.to_datetime(dfDsh['date'])\n",
    "\n",
    "# dfDsh_full = dfdates.merge(dfDsh, on=['date', 'driverId'], how='left')\n",
    "\n",
    "# dfDsh_full['driverId'] = dfDsh_full['driverId'].ffill()\n",
    "# dfDsh_full['activeDays'] = dfDsh_full.groupby('driverId')['activeDays'].ffill().fillna(0)\n",
    "\n",
    "# dfDsh_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfDsh_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfDays = dfTxn_full.merge(dfLeaves_full, on=['date', 'driverId'], how='left').merge(dfDsh_full, on=['date', 'driverId'], how='left')\n",
    "# dfDays['date'] = pd.to_datetime(dfDays['date']).dt.date\n",
    "\n",
    "# dfDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily = dailyDf.merge(dfDays, on=['date', 'driverId'], how='left')\n",
    "\n",
    "# daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall = overallDf.merge(dfDays, on=['date', 'driverId'], how='left')\n",
    "\n",
    "# overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLeaves = prodFetch(f\"\"\"select driverId, date, 1 as nonOpsFlag FROM driverLeaves force index(driverId) WHERE deletedAt IS NULL AND driverId LIKE 'D%' AND date between '{start_date}' and '{yesterday_date}'\"\"\")\n",
    "\n",
    "dfLeaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTxn = prodFetch(f\"\"\"SELECT distinct driverId, date from transactions force index(transaction_date_index) WHERE date between '{start_date}' and '{yesterday_date}' and deletedAt IS NULL and driverId LIKE 'D%' \"\"\")\n",
    "\n",
    "dfTxn['opsFlag'] = 1\n",
    "\n",
    "dfTxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf = dailyDf.merge(dfTxn, on=['driverId', 'date'], how='left').merge(dfLeaves, on=['driverId', 'date'], how='left')\n",
    "\n",
    "dailyDf.fillna(0, inplace=True)\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = overallDf.merge(dfTxn, on=['driverId', 'date'], how='left').merge(dfLeaves, on=['driverId', 'date'], how='left')\n",
    "overallDf.fillna(0, inplace=True)\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(start=start_date, end=yesterday_date)\n",
    "\n",
    "buffer_days = 90\n",
    "min_date = start_date - timedelta(days=buffer_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDsh = prodFetch(f\"\"\"select driverId, date from driverStatusHistories where driverId like 'D%' and date >= '{min_date}' and deletedAt is null and status in ('active')\"\"\")\n",
    "\n",
    "dfDsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrxn = prodFetch(f\"\"\"select distinct driverId, date from transactions where driverId like 'D%' and date >= '{min_date}' and deletedAt is null\"\"\")\n",
    "\n",
    "dfTrxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "dfDsh['date'] = pd.to_datetime(dfDsh['date'])\n",
    "dfTrxn['date'] = pd.to_datetime(dfTrxn['date'])\n",
    "\n",
    "for current_date in date_range:\n",
    "    window_start = current_date - timedelta(days=90)\n",
    "    \n",
    "    active = dfDsh[(dfDsh['date'] > window_start) & (dfDsh['date'] <= current_date)]\n",
    "    active_count = active.groupby('driverId').size().reset_index(name='activeDays')\n",
    "\n",
    "    ops = dfTrxn[(dfTrxn['date'] > window_start) & (dfTrxn['date'] <= current_date)]\n",
    "    ops_count = ops.groupby('driverId').size().reset_index(name='opsDaysCount')\n",
    "\n",
    "    merge = active_count.merge(ops_count, on='driverId', how='outer').fillna(0)\n",
    "    merge['date'] = current_date\n",
    "    merge['ops/Active'] = merge['opsDaysCount'] / merge['activeDays']\n",
    "    merge['ops/Active'] = merge['ops/Active'].replace([np.inf, -np.inf, np.nan], 0)\n",
    "\n",
    "    result.append(merge)\n",
    "\n",
    "final_df = pd.concat(result).reset_index(drop=True)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf['date'] = pd.to_datetime(dailyDf['date'])\n",
    "final_df['date'] = pd.to_datetime(final_df['date'])\n",
    "\n",
    "dailyDf = dailyDf.merge(final_df, on=['driverId', 'date'], how='left')\n",
    "dailyDf.fillna(0, inplace=True)\n",
    "\n",
    "dailyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf['date'] = pd.to_datetime(overallDf['date'])\n",
    "final_df['date'] = pd.to_datetime(final_df['date'])\n",
    "\n",
    "overallDf = overallDf.merge(final_df, on=['driverId', 'date'], how='left')\n",
    "overallDf.fillna(0, inplace=True)\n",
    "\n",
    "overallDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dailyDf.columns)\n",
    "print(overallDf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = [\n",
    "    'isDefaulter', 'nonOpsDays', 'numberOfSwaps',\n",
    "    'smartCardTransactionCount', 'vpaTransactionCount',\n",
    "    'smartCardOccupancyFlag', 'totaltickets',\n",
    "    'opsFlag', 'nonOpsFlag'\n",
    "]\n",
    "\n",
    "float_cols = [\n",
    "    'cashWallet', 'pointsWallet', 'penaltyWallet',\n",
    "    'upiCollection', 'penalty', 'netGMV',\n",
    "    'walletCashUsed', 'pointUsed', 'ops/Active'\n",
    "]\n",
    "\n",
    "date_cols = ['date', 'liveDate']\n",
    "\n",
    "all_cols = [\n",
    "    'driverId', 'date', 'operatorId', 'zoneId', 'vehicleType', 'clientId',\n",
    "    'liveDate', 'cashWallet', 'pointsWallet', 'penaltyWallet',\n",
    "    'isDefaulter', 'nonOpsDays', 'upiCollection', 'penalty', 'netGMV',\n",
    "    'walletCashUsed', 'pointUsed', 'numberOfSwaps',\n",
    "    'smartCardTransactionCount', 'vpaTransactionCount', 'status',\n",
    "    'smartCardId', 'smartCardOccupancyFlag', 'StickyPartnerId',\n",
    "    'totaltickets', 'mobile', 'alternateMobile', 'callingNumber', 'opsFlag',\n",
    "    'nonOpsFlag', 'ops/Active'\n",
    "]\n",
    "\n",
    "string_cols = list(set(all_cols) - set(int_cols) - set(float_cols) - set(date_cols))\n",
    "\n",
    "for col in int_cols:\n",
    "    dailyDf[col] = dailyDf[col].fillna(0).astype(int)\n",
    "    overallDf[col] = overallDf[col].fillna(0).astype(int)\n",
    "\n",
    "for col in float_cols:\n",
    "    dailyDf[col] = dailyDf[col].astype(float)\n",
    "    overallDf[col] = overallDf[col].astype(float)\n",
    "\n",
    "for col in date_cols:\n",
    "    dailyDf[col] = pd.to_datetime(dailyDf[col])\n",
    "    overallDf[col] = pd.to_datetime(overallDf[col])\n",
    "\n",
    "for col in string_cols:\n",
    "    dailyDf[col] = dailyDf[col].astype(str)\n",
    "    overallDf[col] = overallDf[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
